<!DOCTYPE html>
<html>
<head>
<title>body-based [interface ecology lab]</title>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<link href="../../includes/ecologylab.css" rel="stylesheet">
<link rel="icon" href="../../images/ecologylab-16.png" type="image/png">
<link rel="shortcut icon" href="../../images/ecologylab-16.ico" type="image/ico">
<link rel="icon" href="../../images/ecologylab-16.png" type="image/png">

<script type="text/javascript">
  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-20550827-1']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();
</script>
<script type="text/javascript" src="../../includes/ecologylab.js"></script>
</head>
<body>

<div style="" class="rootcontainer">

<div class="header">
<a href="../../index.html" ><img class="logo" src="../../images/interfaceEcologyLabLogos7-2004-2WhiteMatte.png"> </a>
<div class="title smaller">
body-based interaction
</div>
<span onclick="toggleNav(this);" id="triple">&equiv;</span>
</div>

<div id="navigation_container">
  <a class="navlink" href="../../index.html">home</a>
  <a class="navlink" href="../../define.html">define</a>

  <a class="navselected" href="../index.html">research</a>
  <div class="sub_block">
    <a class="subnavselected" href="./index.html">body-based</a>
    <a class="subnavlink" href="../ideation.html">ideation</a>
    <a class="subnavlink" href="../livemedia/index.html">live media</a>
    <a class="subnavlink" href="../curation.html">curation</a>
    <a class="subnavlink" href="../websemantics/index.html">web semantics</a>
    <a class="subnavlink" href="../games.html">games</a>
  </div>

  <a class="navlink" href="../../people/index.html">people</a>
  <a class="navlink" href="../../courses/index.html">teaching</a>
  <div class="subblock">
        <a class="nav_nowrap" target="_blank"
         href="https://facebook.com/ecologylab"><img style="vertical-align:text-bottom" src="https://facebook.com/favicon.ico"/>ollow</a>
  </div>
</div>

<!--The continuing development of miniature low-power sensors marks a
historic period in the relationships between people and machines. On
the one hand, these devices raise the terrible spectre of invasive
surveillance. At the same time, they create the potential for new
modes of interaction and communication. New interface ecosystems can
sense, recognize, respond to, and represent nuances in
in our environments and in our bodies.
They spawn new forms of communication.
The Interface Ecology Lab is emphasizing processes of
human expression and social interaction as we develop sensory
interfaces that involve embodied awareness of the human body and the physical world.
</p>

<p>
We integrate sensor
networks with methods from ubiquitous, pervasive, and wearable
computing, real time embedded systems,
psychophysiology, pattern recognition, performance studies, installation art,
conceptual art, and context-aware,
location-aware and embodied HCI. 
The 
<a target="_blank" class="destination" href="http://www.cypress.com/?id=1353">Cypress PSoC (Programmable System on a Chip)</a>
is a favorite building block, because it enables flexible configuration of hardware interfaces to sensors, a/d and d/a conversion, and signal processing, speeding prototype development.
</p>

<p>
The new sensory forms of embodied interactivity are based in our
physical and corporeal beings. The forms we are creating
include mixed-reality games,
installations, body-based affordances, and aesthetic design environments.

</p>-->

<p>
	<img src="../images/artchi.jpg" width="280" class="illustrate_left"/>

We humans experience the world through our bodies.
We form understandings for how to interact with environments through bodily senses.
The advent of new low-cost, low-power sensing technologies create the potential for new
modes of interaction and communication.
New interface ecosystems can sense, recognize, respond to, and represent nuances
in our environments and in our bodies. 
The Interface Ecology Lab is emphasizing processes of human expression, ideation, and social interaction as we develop sensory interfaces that 
involve embodied awareness of the human body and the physical world.
</p>
<img src="../images/bodyBasedDiagramming.jpg" width="350" class="illustrate" style="clear:left;"/>
<h3>body-based diagramming</h3>
<p>

<!--
<p>
We integrate body-based sensing with methods from creativity support environments, interaction design, ubiquitous
computing, psychophysiology, pattern recognition, performance studies, installation art, conceptual art, and context-aware,
location-aware and embodied HCI. 
</p>--> 





We are investigating how body-based interfaces support creativity, expression, and design.
We are particularily interested in the context of <a class="destination" href="ideation.html" target="_blank">ideation</a>
in design.
Design processes are supported by embodied representations,
including gestures, tangibles, and diagrams, which
have been found to help people think.
</p>
<p>
A <i>diagram</i> is a design thinking tool that enables and stimulates
imagination, facilitating conceptualization. Diagrams
mediate exploration of relationships between concepts, using
ambiguous visual representations to foster varied, flexible
interpretations. We use a form of diagramming called, <a href="../curation.html" target="_blank" class="destination">free-form curation</a>.
</p>
<p>
We are developing a new creativity support environment, Body-based IdeaM&Acirc;CH&Eacute;, that enables designers 
to express, collect, organize, and reflect upon ideas using pen + touch interaction. 
The kinematic chain model for bimanual interaction serves as a basis for the design of new gestural interaction techniques. 
In the kinematic chain model, interactions with the non-preferred hand function as a frame of reference for actions with the preferred hand. 
For example, when drawing on paper, the non-preferred hand positions the paper for drawing with a pencil in the preferred hand. 
We seek to support expression and ideation by enabling designers to fluidly switch between and manipulate parameters of diagram transformations with their hands.
</p>
<p>

<h3>collaborative design ideation</h3>
<img src="../images/CollaborativeDesign2.jpg" width="200" class="illustrate_left"/>
  We are investigating landscape architecture students' individual and collaborative ideation practices and experiences in design studio education. We have won a <a href="https://www.microsoft.com/en-us/research/project/surface-hub-for-research/" target="_blank" class="destination">Surface Hub for Research</a> grant from Microsoft Research, for developing new and collaborative pen+touch interaction techniques and integrating them into a design environment in order to show how Surface Hub can provide a basis for advancing design ideation.  
</p>
<p>
To effectively build interactive environments that support the needs of visual designers, we need to understand designers' current processes of printing, drawing, and pinning media as they develop ideas, in project development, and during critique sessions in studios. We are looking at the media they use, the interactions their hands and bodies become involved in with that media, and how they interweave individual and collaborative work to create design products. 
</p>
<p>
Our larger goal is to support individual and collaborative ideation in design studios using interactive surfaces. We will design and develop interaction techniques, integrated with software infrastructure, to support multiple users collaborating synchronously on Microsoft Surface Hub.
</p>
<p>
We have also worked on innovative hardware designs to support body-based interaction, such as the 
<a href="zerotouch/index.html" class="destination" target="_blank">ZeroTouch</a> sensor, which detects visual hulls, in a plane, in free-air or in proxmity to a surface.
</p>

<!-- CROSS SURFACE INTERACTION -->
<!--
<img src="../images/crossSurfaceSetup.png" width="200"
 style="float: left; margin-right: 10px; margin-top: 4px; margin-bottom: 5px;"/>


<span class="sub_title">cross-surface interaction</span><br/>

<p>
We are developing new methods for <i>cross-surface interaction</i>, that is interaction across multi-touch surfaces.
Embodied cognition is key. We inform our designs through the use of <i>culturally based design</i>, a method that draws
from the embodied experiences of activities embedded in culture. By mimicking these well established activities, interactions activate 
people's embodied mental models. This makes the interactions intuitive and familiar, as they draw from prior experiences. 
Our culturally based designs draw a variety of contexts, ranging from card playing to sharing and exchanging photographs.</p>
</p>

<p>
Our long term objective is to give people  
interactive experiences in which embodied gestures 
performed by the human hand are mapped to actions in 
ways that are natural, meaningful and intuitive.
</p>


<p style="clear: left; padding-top:20px;">
We have also worked on innovative hardware designs to support body-based interaction, such as the 
<a href="zerotouch/index.html" class="destination" target="_blank">ZeroTouch</a> sensor, which detects visual hulls, in a plane, in free-air or in proxmity to a surface.
-->


<!--
<h4 style="margin-top:55px;">projects</h4>

<div class="feature">
<a href="zerotouch/index.html">
<img
  class="illustrate_left" style="border: 0px"
  width="144"
  height="100"
  src="zerotouch/images/zerotouchMeshIcon2.png"></a>
<a href="zerotouch/index.html" class="checkthis">ZeroTouch</a> is a high performance multi-point sensor that detects visual hulls, within a plane. Applications include free-air interaction, person tracking in rooms, automobile windshields, hover sensing (e.g., over capacitive multi-touch), and multi-touch.
</div>

<div class="feature">
<a href="https://github.com/ecologylab/EcoTUIODriver" target="_blank">
<img
  class="illustrate_left"
  width="144"
  height="100"
  src="../images/1x1.gif"></a>
<a class="destination" href="https://github.com/ecologylab/EcoTUIODriver" target="_blank">EcoTUIODriver</a> is our own 
<i>open source</i> TUIO to Windows 7/8 Touch bridge. It uses virtual touch drivers and TUIO bridge services to convert TUIO
 touch messages to Windows touch events. EcoTUIODriver supports TUIO bridging on up to 5 monitors simultaneously. 
The EcoTUIODriver source can be downloaded from
<a class="destination" href="https://github.com/ecologylab/EcoTUIODriver" target="_blank">github</a> or you can pull down 
the installer
<a class="destination" href="https://github.com/downloads/ecologylab/EcoTUIODriver/EcoTuioDriver.exe" target="_blank">here</a>.</p> 
</div>
-->



<h2>publications and exhibitions</h2>

<div class="feature">
  <a class="a_illustrate" href="../../research/publications/webbInterstices.pdf" target="_blank">
  <img
    class="illustrate_left"
    width="144"
    height="100"
    src="../../research/images/interstices.png" />
  </a>
  <div class="text_paper">
  Webb, A. M., Fowler, H., Kerne, A., Newman, G., Kim, J.-H., Mackay, W. E.
  <a href="../../research/publications/webbInterstices.pdf" class="pub_destination" target="_blank">Interstices: Sustained Spatial Relationships between Hands and Surfaces Reveal Anticipated Action</a>,
  <i>Proc. of ACM CHI Conference on Human Factors in Computing Systems 2019</i>, Glasgow, UK. 
  <!--<a class="doi" href="https://doi.org/10.1145/3290605.3300818" target="_blank">
    https://doi.org/10.1145/3290605.3300818
  </a>-->
  </div>
</div>

<div class="feature">
  <a class="a_illustrate" href="https://dl.acm.org/authorize?N677778" target="_blank">
  <img
    class="illustrate_left"
    width="144"
    height="100"
    src="../../research/images/layerFish.png" />
  </a>
  <div class="text_paper">
  Webb, A.M., Kerne, A., Brown, Z., Kim, J.H., Kellogg, E.,   
    <a href="https://dl.acm.org/authorize?N677778" class="pub_destination" target="_blank">LayerFish: Bimanual Layering with a Fisheye In-Place</a>, 
    <i>Proc. ACM International Conference on Interactive Surfaces and Spaces 2016</i>, Niagara Falls, Canada. [28%].
    <a class="doi" href="http://dx.doi.org/10.1145/2992154.2992171" target="_blank">http://dx.doi.org/10.1145/2992154.2992171</a>
  </div>
</div>

<div class="feature">
  <a class="a_illustrate" href="https://dl.acm.org/authorize?N677770" target="_blank">
  <img
    class="illustrate_left"
    width="144"
    height="100"
    src="../../research/images/guiardAbidingTouch.png" />
  </a>
  <div class="text_paper">
    Webb, A.M., Pahud, M., Hinckley, K., Buxton, B.,   
    <a href="https://dl.acm.org/authorize?N677770" class="pub_destination" target="_blank">Wearables as Context for Guiard-abiding Bimanual Touch</a>, 
    <i>Proc. ACM Symposium on User Interface Software and Technology 2016</i>, Tokyo, Japan.  [20.6%].
    <a class="doi" href="http://dx.doi.org/10.1145/2984511.2984564" target="_blank">http://dx.doi.org/10.1145/2984511.2984564</a>
  </div>
</div>

<div class="feature">
	<a class="a_illustrate" href="https://dl.acm.org/authorize?N677773" target="_blank"><img
	  class="illustrate_left"
	  width="144"
	  height="100"
	  src="../images/penTouchDiagramming.png" /></a>
  <div class="text_paper">
    Webb, A.M.,
    <a class="pub_destination" href="https://dl.acm.org/authorize?N677773" target="_blank">
    Pen + Touch Diagramming to Stimulate Design Ideation</a>,
    <i>Proc. ACM Creativity and Cognition 2015</i>, Graduate Symposium, 331-332.
    <a class="doi" href="http://dx.doi.org/10.1145/2757226.2764766" target="_blank">http://dx.doi.org/10.1145/2757226.2764766</a>
	</div>
</div>

<div class="feature">
  <a class="a_illustrate" href="https://dl.acm.org/authorize?N677785" target="_blank">
    <img
	  class="illustrate_left"
	  width="144"
	  height="100"
    src="../images/artchi_thumb.jpg" />
  </a>
	<div class="text_paper">
	Lupfer, N., Hamiliton, W. Webb, A.M., Linder, R., Edmonds, E., and Kerne, A.,
	<a class="pub_destination" href="https://dl.acm.org/authorize?N677785" target="_blank">
	The Art.CHI Gallery: An Embodied Iterative Curation Experience</a>,
	Interactivity Exhibit,
	<i>Proc CHI 2015 EA</i>. <!--, 1165-1170 [%].-->
  <a class="doi" href="http://dx.doi.org/10.1145/2702613.2725457" target="_blank">http://dx.doi.org/10.1145/2702613.2725457</a>
  </div>
	
</div>

<div class="feature">
<a class="a_illustrate" href="../../research/publications/webbEmbodiedDiagrammingGestures.pdf" target="_blank">
<img
  class="illustrate_left"
  width="144"
  height="100"
  src="../../research/images/embodiedDiagrammingGestures.png" />
</a>
<div class="text_paper">
Webb, A.M. and Kerne, A.,
<a class="pub_destination" href="../../research/publications/webbEmbodiedDiagrammingGestures.pdf" target="_blank">Embodying Diagramming through Pen + Touch Gestures</a>,
<i>CHI 2014 Workshop: Gesture Interaction Design: Communication and Cognition</i>, Toronto, Canada.
</div>
</div>

<div class="feature">
<a class="a_illustrate" href="https://dl.acm.org/authorize?N677787" target="_blank">
<img
  class="illustrate_left"
  width="144"
  height="100"
  src="../../research/images/trans-surface.png" />
</a>
<div class="text_paper">
Fei, S., Webb, A.M., Kerne, A., Qu, Y., and Jain, A.,
<a href="https://dl.acm.org/authorize?N677787" target="_blank">Peripheral Array of Tangible NFC Tags: Positioning Portals for Embodied Trans-Surface Interaction</a>,
<i>Proc. Interactive Tabletops and Surfaces 2013</i>, [29%].
<a class="doi" href="http://dx.doi.org/10.1145/2512349.2512820" target="_blank">http://dx.doi.org/10.1145/2512349.2512820</a>
<a target="_blank" class="pub_destination" href="https://www.youtube.com/watch?v=Z-rTTJAH2-k">[video]</a>
</div>
</div>


<div class="feature">
<a class="a_illustrate" href="../publications/RtsUist2012.pdf" target="_blank">
<img
  class="illustrate_left"
  width="144"
  height="100"
  src="../images/rtsUISTPic.png" />
</a>
<div class="text_paper">
Hamilton, W., Kerne, A., and Robbins, T.,
<a href="../publications/RtsUist2012.pdf" target="_blank">High-Performance Pen + Touch Modality Interactions: A Real-Time Strategy Game eSports Context</a>,
<i>Proc. UIST 2012</i>, Cambridge, MA, USA, OCT 2012.
</div>
</div>

<div class="feature">
<a class="a_illustrate" href="../publications/kerneHamiltonToupsTransSurfaceCBDcscw2012.pdf" target="_blank"><img
  class="illustrate_left"
  width="144"
  height="100"
  src="../images/rummy_tile.png" />
</a>
<div class="text_paper">
Kerne, A., Hamilton, W., Toups Dugas, P. O.
<a href="../publications/kerneHamiltonToupsTransSurfaceCBDcscw2012.pdf" target="_blank">Culturally Based Design: Embodying Trans-Surface Interaction in Rummy
</a>
<i>Proc CSCW 2012</i>, in press [top 9%].
</div>
</div>

<div class="feature">
<a class="a_illustrate" href="../publications/icmi2011ComparingMultitouch-damaraju.pdf" target="_blank">
<img
  class="illustrate_left"
  width="144"
  height="100"
  src="../images/comparingMultitouchInterface.png" />
</a>
<div class="text_paper">
Damaraju, S., Kerne, A., 
<a href="../publications/icmi2011ComparingMultitouch-damaraju.pdf" target="_blank">
Comparing Multi-Touch Interaction Techniques for Manipulation of an Abstract Parameter Space</a>, accepted to <i>Proc. International Conference on Multimodal Interaction (ICMI) 2011</i>.
</div>
</div>

<div class="feature">
<a class="a_illustrate" href="../publications/moeller-zerotouch.pdf" target="_blank"> 
<img
  class="illustrate_left"
  width="144"
  height="100"
  src="../images/zerotouchicon.jpg" />
</a>
<div class="text_paper">
Moeller, J. and Kerne, A.,
<a href="../publications/moeller-zerotouch.pdf" target="_blank">
ZeroTouch: A Zero-Thickness Optical Multi-Touch Force Field</a>, CHI Interactivity -- 
<i>Extended Abstracts of the 29th International Conference on Human factors in computing systems, May 7-12, 2011, Vancouver, BC, CA.</i>
[<a target="_blank" class="destination" href="../images/zerotouch.mov">video</a>]
</div>
</div>

<div class="feature">
<a class="a_illustrate" href="../publications/intangibleCanvas.pdf" target="_blank"> 
<img
  class="illustrate_left"
  width="144"
  src="../images/intangibleicon.png" />
</a>
<div class="text_paper">
Moeller, J., Lupfer, N., Hamilton, B., Lin, H., Kerne, A.,
<a href="../publications/intangibleCanvas.pdf" target="_blank">
intangibleCanvas: Free-Air Finger Painting on a Projected Canvas </a>, CHI Works-in-Progress -- 
<i>Extended Abstracts of the 29th International Conference on Human factors in computing systems, May 7-12, 2011, Vancouver, BC, CA.</i>
</div>
</div>

<div class="feature">
<a class="a_illustrate" href="../publications/sp276-moeller.pdf" target="_blank"> 
<img
  class="illustrate_left"
  width="144"
  height="100"
  src="../images/happyfuntouch-icon.png" />
</a>
<div class="text_paper">
Moeller, J. and Kerne, A.,
<a href="../publications/sp276-moeller.pdf" target="_blank">
Scanning FTIR: Unobtrusive Multi-Touch Sensing through Waveguide Transmissivity Imaging</a>, 
<i>Proceedings of the 4th International Conference on Tangible, Embedded and Embodied Interaction</i>, 
Cambridge, MA, Janurary 25-27 2010.
</div>
</div>

<div class="feature">
<a class="a_illustrate" href="../publications/tabletop08-posterAbstract1-6.pdf" target="_blank">
<img
	class="illustrate_left"
	width="144"
	height="100"
	src="../images/gestureTabletop08Icon.png"/>
</a>
<div class="text_paper">
Damaraju, S., Kerne, A. 
<a href="../publications/tabletop08-posterAbstract1-6.pdf" target="_blank">
Multitouch Gesture Learning and Recognition System.
</a>,  
<i>Extended Abstracts of IEEE Workshop on Tabletops and Interactive Surfaces</i>, 
Amsterdam, Netherlands, 1-3 October 2008.
</div>
</div>

<div class="feature">
<a class="a_illustrate" href="https://dl.acm.org/authorize?N677798" target="_blank">
<img class="illustrate_left" width="144" height="100"
 src="../images/cb/choreographicButtonsIcon.jpg"></a>
<div class="text_paper">
Webb, A., Kerne, A., Koh, E., Joshi, P., Park, Y., Graeber, R., 
<a href="https://dl.acm.org/authorize?N677798" target="_blank">
Choreographic Buttons: Promoting Social Interaction through Human Movement and Clear Affordances</a>, <i>Proc ACM Multimedia 2006</i>, 451-460 [16%].
<a class="doi" href="http://dx.doi.org/10.1145/1180639.1180731" target="_blank">http://dx.doi.org/10.1145/1180639.1180731</a>
</div>
</div>

<div class="feature">
<a
class="a_illustrate" href="../publications/censorChairMM05.pdf" target="_blank"><img class="illustrate_left" width="144" height="100"
 src="../images/censorChairIcon.jpg"></a>
 <div class="text_paper">
Toups Dugas, P. O., Overby, K., Kerne, A., Graeber, R., Cooper, T., Alley, E.,
<i>Censor Chair</i>,
ACM SIGCHI Intl Conf on Advances in Computer Entertainment, June 2006,
Hollywood.
<br/>
<br/>
Alley, E. Cooper, T., Graeber, R., Kerne, A., Overby, K., Toups Dugas, P. O.,
<a
 href="../publications/censorChairMM05.pdf" target="_blank">Censor Chair: Exploring Censorship and Social Presence
through Psychophysiological Sensing</a>, <i>Proc ACM Multimedia 2005</i>, 922-929.
</div>
</div>

<div class="feature">
<a
class="a_illustrate" href="../publications/playasHomelandMirageMM05.pdf" target="_blank"><img class="illustrate_left" width="144" height="100"
 src="../images/welcomeToPlayasIcon.jpg"></a>
 <div class="text_paper">
Stenner, J., Kerne, A., Williams, Y.,
Playas: Homeland Mirage, <i>ISEA/Zero One</i>, Aug 2006: San Jose.
<br/>
<br/>
Stenner, J., Kerne, A., Williams, Y.,
<a
 href="../publications/playasHomelandMirageMM05.pdf" target="_blank">Playas: Homeland Mirage</a>, <i>Proc ACM Multimedia 2005</i>, 1057-1058.
</div>
</div>

<div class="feature">
<div class="text_paper">
Schiphorst, T., Kozel, S., Andersen, K., Jaffe, N., Mah, S., Kerne, A., Lovell, R., Tolmie, J.,
<a href="http://deaf.v2.nl/deaf/03/132-147-135-9-125-244-30-205-202-104-220-62-165-100-81-117.py" target="_blank">whisper</a>.
<i>Dutch Electronic Arts Festival (DEAF) 2003</i>, Rotterdam, The
Netherlands.
</div>
</div>

<br/><br/>

</div>

</body> </html>
